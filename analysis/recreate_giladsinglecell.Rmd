---
title: "Recreate Gilad lab single cell"
author: "Brana Mittleman"
date: "January 11, 2017"
output: html_document
---

```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

```{r knitr-opts-chunk, include=FALSE}
```

**Last updated:** `r Sys.Date()`

**Code version:** `r workflowr::extract_commit(".", 1)$sha1`


I will use this to document my recreating of the single cell data processing from http://jdblischak.github.io/singleCellSeq/analysis/process-samples.html.  

###General information for committing changes to the git hub site: 

```
git add analysis/*.Rmd
git commit -m "message"
```
Run `wflow_commit()` in R terminal.

```
git push origin master
```


###Programs for analysis 
First I need to document the programs I will need to add to midway to complete the processing recreation. 

- umitools v2.1.1
-fastqc **on midway- not told you need it in github commands!**

- sickle version 1.33 **used make function**
- Subread version 1.5.0-p1 **on midway**
- samtools version 0.1.18-dev (r982:313) **version 1.2 on midway**
- UMI-tools (version e0ade5d)
- featureCounts version 1.5.0-p1 **i think this is a part of subread**


To use modules on midway:
```
module load [name]
module list #show loaded modules 
module unload [name]

```



###Moving the files from pps to midway 
First I copy the first individual, first plate from spudhead to gateway.  
```
scp 19098.1* pps-gateway:
```
Move the files to midway:
```
scp 19098.1* brimittleman@midway.rcc.uchicago.edu:
```
Remove the files from gateway: 
```
rm 19098.1*
```
Move to the midway cluster and move the files to a directory in scratch. 
```
mv 19098.1* scratch-midway/19098.1 
```

###Create genome for mapping
Create a code directory for scripts. 
```
mkdir code
```
Copy contents of create-genome.sh from John's github to a script in my own directory using vi. 

```
vi create-genome.sh
```
Use escape then :wq to write out file.  Make a file executable using the following:
```
chmod 700 <file>
```

Run each script seperatly:
```
sbatch --mem=8g --wrap="create-genome.sh genome"
```
This submits the job.To view the job I use this command.
```
squeue -u brimittleman
```

Try this one command at a time in a genome directory. This is buiding the reference index genome for later transcriptome mapping. 
```
mkdir genome
cd genome
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/chromosomes/chr{1..22}.fa.gz
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/chromosomes/chr{X,Y,M}.fa.gz
gunzip chr*fa.gz
wget http://tools.invitrogen.com/downloads/ERCC92.fa
#load subread
subread-buildindex -o combined *.fa
```

I need to move data and scripts to the gilad project folder. scratch is not permanant.

```
mkdir /project/gilad/brimittleman
```


###Run FastQC
Create run-fastqc.sh file:

```
vi run-fastqc.sh
chmod 700 run-fastqc.sh #make file executable
```
Load the fastqc package and submit the job to the cluster: 
```
sbatch --mem=2g run-fastqc.sh ../19098.1/19098.1.A09.GGTATTCA.L003.R1.C6WURACXX.fastq.gz #Run this command for eachfastq. 
```

**Each new Session: load fastqc, subread, samtools. **  

Problem at this step:  

 - Files -19098.1.C08.TTGGTACA.L002.R1.C6WYKACXX.fastq.gz to the end are empty. Cannot run the fast qc on these.   
 
 - To check this I go back to the pps spudhead and look at the files using.  
 
```
 zcat file |wc -l
```
 - The files are correct on spudhead. I will recopy them over to midway. 


Return to workflow:  
```
ls fastqc/ | wc -l
#201

```

###Run Fastqc 2
Create run-fastqc.sh file:  

```
vi run-fastqc.sh
chmod 700 run-fastqc.sh #make file executable
```
I am redoing the run fastqc step using a code that can run all of the files at the same time. I created a file called wrap-sbatch.sh that will run the run-fastqc.sh script in a loop over all of the fastq files. To complete this task all of the .sh scripts have to be in the same directory as the data. I also added a module load command for the fastqc module within the run-fastqc.sh script for completness. I run the sbatch for all of the fastq files using the following command:  
```
./wrap_sbatch.sh *fastq.gz
```
This submits a job per file and I can check which jobs are running using the squeue - u command.  

```
ls fastqc/ | wc -l
```
I have 447 lines in the fastqc directory. 

###Trim UMI  
Create the trim.sh script using the code from github in the code folder. Run this on all the files by making a script like the wrap_sbatch.sh one above.   
```
./wrap_trim.sh *fastq.gz
ls trim/*fastq.gz | wc -l
#224 this is one file per fastq.gz file in the data.  
ls invalid/*fastq.gz | wc -l
#no invalid files were created 
```
###Quality trim 3' end of reads  
This uses sickle.sh to perform quality trimming of the 2' end of the reats. I am using sickle-1.33. I will create a new wrap function for this segmant of code as well that will wrap the sickle.sh code. This code is within the trim folder because those are the files that I will run the command on.  

```
./wrap_sickle.sh *fastq.gz
ls sickle/*fastq.gz | wc -l
#224
```

###Map genome

I will use the map-subujunc.sh script to map the fastq files to the genome. I will create a new wrap script for this job as well. In the wrap I will load the subread module. I am copying the genome directory to sickle directory so all of the paths are easy.    

```
./wrap_map.sh *fastq.gz
ls bam/*bam | wc -l
#224
```

The bam files were empty because I exceeded the memory needed. I added --mem=12G to the wrap script. I also changed it to the mstephen partition with --partition=mstephens and changed account with --account=pi-mstephens.



###Process bam files
I will use the process-bam.sh script to process the bam files. I will create a new wrap script for this and will have it load samtools module in the script. All of this takes place in the bam folder.  



##Re-organize my files

I am reorganizing my data and code according to John's organization. I created a single cell directory in my home directory and in my project/gilad directory. I keep all of the scripts I have created in a code directory within the home single cell directory and edit my path variable to include this. The file containing my path directort in called .bashrc and is in the home directory. The fastq files were moved to a fastq directory within the single cell directory in the project folder. This way I am now keeping scripts in my home folder and have all large data in my project folder. I run jobs from the single cell directory in my project/gilad folder.  

###Umi trim
-I need to update the trim.sh script for the umi_tools extract command.  


 The code below works but it puts the output in the directory I ran the script in instead of the trim folder. Going to wrap it for all the files anyway:
 ```
 umi_tools extract --stdin=$FILE  --bc-pattern=[NNNNNGGG]  -L extract.log --stdout=$BASE.trim.fastq.gz \
  -E $INVALID_DIR/$BASE.invalid.fastq \
  2> $INVALID_DIR/$BASE.trim.stats.txt | gzip -c > $OUTDIR/$BASE.trim.fastq.gz
```

I added code to increase memory and run the code in the mstephens partition to the wrap_trim.sh script and ran it on all of the fastq files using the following code in the project single cell directory. 

```
wrap_trim.sh fastq/*fastq.gz
```

Only works when I run this in the fastq directory. The output files have *.trim.fastq.gz. I will move these to the trim directory and more the trim directory out of the fastq directory.  

###Sickle 
Run the same wrap sick command within the trim directory. I will move this out when it is done. 


###Map genome 
I am copying the genome directory to sickle directory so all of the paths are easy. Run the following command in the sick directory. I will more the files out later.  

```
wrap_map.sh *trim.sickle.fastq.gz
```

###Process Bam files
I copied the code from process-bam.sh and created the same file in my code directory. I will also make a wrap script for this code. I run these commands in the bam folder and will move them out after.

```
wrap_processbam.sh *trim.sickle.bam
```
Check for processense of intermediate files output during sorting.
```
ls bam-processed/*sorted*0*bam
```
No such files have been created.  

###Combine Single cells
I will run this code 
```
#!/bin/bash
module load samtools
mkdir -p bam-combined
mkdir -p ~/log/combine
for IND in 19098 #19101,19239
do
  for REP in 1
  do
    for ROW in {A..F}
    do
      for COL in {1..12}
      do
        ID=`printf "%s.%s.%s%02d\n" $IND $REP $ROW $COL`
        echo $ID
        echo -e "#!/bin/bash\nsamtools merge bam-combined/$ID.trim.sickle.sorted.combined.bam bam-processed/$ID*trim.sickle.sorted.bam" > tmp-$ID.sh
        sbatch tmp-$ID.sh
      done
    done
  done
done
```
Files are now in bam-combined. There are 72 files.
**Missing G and H from the beginning. once I have pipeline working maybe I will go back and add these files**

Index each merged sample. Using the following code in a script called index.sh.  


```

module load samtools
for IND in 19098 #19101,19239
do
  for REP in 1
  do
    for ROW in {A..F}
    do
      for COL in {1..12}
      do
        ID=`printf "%s.%s.%s%02d\n" $IND $REP $ROW $COL`
        echo $ID
        echo -e "#!/bin/bash\nsamtools index bam-combined/$ID.trim.sickle.sorted.combined.bam" > tmp2-$ID.sh
        sbatch tmp2-$ID.sh
      done
    done
  done
done

```
```
ls bam-combined/*bai | wc -l
```
72
```
cat tmp_index/slurm* | head
```
No output. 

###Remove Duplicate UMIs 
I will now use UMI_tools to remove duplicate UMIs. 
```
#Example code:
umi_tools dedup -I example.bam --output-stats=deduplicated -S deduplicated.bam
#options to think about
--edit-distance-threshold (int) #used 1 in original code 
```

I made the following changes to the original 
```
#prior code
dedup_umi.py --method="directional-adjacency" --edit-distance-threshold=1 \
  -I $FILE -v 0 -S $OUTDIR/$BASE.rmdup.bam
#new code using UMI_tools 
umi_tools dedup  -I $FILE --output-stats=$OUTDIR/dedup_outputstat  -S $OUTDIR/$B
ASE.rmdup.bam
```

Created a wrap script called wrap_dedup.sh.  Run this on the processed and combined samples. 

In the bam-combined folder.  
```
 wrap_dedup.sh *bam
```
To run this in the processed folder not including the bulk I use. 
```
wrap_dedup.sh `ls -1 *.bam | grep -v bulk`  
```
The umi_tools module is only on midway 1. I need to specify the sandyb partition in my wrap script to be able to run this code over all of the bam files. 

I created a new script to run the code on more than one directory at a time called 2dir_dedup.sh. I can use similar code to run the counting functions later on files from 2 directories.  

```
#!/bin/bash

module load samtools
module load python

for file in `ls bam-processed/*bam | grep -v bulk`  bam-combined/*bam
do
	sbatch --mem=4G rmdum-umi.sh $file
done
```


###Count reads per gene 
To run this I need to use the create exon script in R to make the necssesary file for the featurecounts tool to use. Do this on midway2 from the genome folder.  

```
Rscript create-exons.R ERCC92.gtf.1 > exons.saf 

```


I created the count-reads-per-gene.sh and a code to run it on 2 directories. It is called 2dir_count.sh. I ran this script in the project single cell directory and the ouput is in counts.   


###Gather total counts  (Skip this section)
I copied gather-total-counts.py code from the github. This section relies on text files I have not created in all of the directories. It is not necessary I do it.

```
gather-total-counts.py > total-counts.txt
```

###Gather summmary counts  
I copy the gather-summary-counts.py script and comment out the lines that check the number of files. Because I only did one plate these numbers would stop the code from running. 

```
gather-summary-counts.py > summary-counts.txt
```  

###Gather gene counts  
Coppy the gather-gene-counts.py from github. Need to comment out lines that check the number or files becasue I am not working with all of the samples. 
```
mkdir -p counts-matrix
gather-gene-counts.py counts-matrix/ counts/*genecounts.txt
```  


In the end I have a directory called counts-matrix with 6 files in it that will be loaded ito R to continue the process. They are:

* molecules-raw-single-per-lane.txt 

* molecules-raw-single-per-sample.txt 

* reads-raw-bulk-per-lane.txt 

* reads-raw-bulk-per-sample.txt

*  reads-raw-single-per-lane.txt

* reads-raw-single-per-sample.txt 
